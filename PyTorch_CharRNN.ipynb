{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqaZvADhVksb"
   },
   "source": [
    "# Following the tutorial to create a Character-Level Recurrent Neural Network\n",
    "*   https://github.com/LeanManager/NLP-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOMUrjYcWGiB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDMDpWQHbLOu"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mswmRwtKVhid"
   },
   "outputs": [],
   "source": [
    "with open('bible.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnNMXYZnYbxi"
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {char: integer for integer, char in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text]) # text in number format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dyVhYZrObGNx"
   },
   "source": [
    "## One-Hot Encoding\n",
    "*   Se faz a transformação em One Hot para poder facilmente gerar os vetores depois para cada caractere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A3cIqi-Z0T6"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot  \n",
    "#Isso vai gerar a matriz de one hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hhiwuj--DP49"
   },
   "source": [
    "## Create Mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoJv4bMNby2a"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "\n",
    "    batch_size = n_seqs * n_steps #Numero de sentencas e numero de caracteres (tamanho das sentenças)\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Mantém um número que não sobram batches inacabados\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYeInyvsclAy"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6Dc40XiJwGv"
   },
   "outputs": [],
   "source": [
    "x,y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "cx6u_az1J-tf",
    "outputId": "53d0a150-0a52-4a20-f430-9cb359229dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:1 In the beginning God created the heaven and th\n",
      "at reddish, and it be\n",
      "shewed to the priest; 13:20 \n",
      "the LORD followed them.\n",
      "\n",
      "6:9 And the armed men wen\n",
      " shew thyself pure; and with the froward\n",
      "thou wilt\n",
      "they were destroyed before the LORD, and before hi\n",
      "tter than an ox or bullock\n",
      "that hath horns and hoo\n",
      "ath from polluting it, and taketh hold\n",
      "of my coven\n",
      ", saith the Lord GOD, be it known\n",
      "unto you: be ash\n",
      ":23 If any man have ears to hear, let him hear.\n",
      "\n",
      "4\n",
      "at after the most\n",
      "straitest sect of our religion I\n"
     ]
    }
   ],
   "source": [
    "for k in x:\n",
    "  text = ''\n",
    "  for i in k:\n",
    "    text = text + int2char[i]\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "mLvkkyZ0Kqa4",
    "outputId": "65891f89-d853-4ed7-e801-7c42cf62614d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":1 In the beginning God created the heaven and the\n",
      "t reddish, and it be\n",
      "shewed to the priest; 13:20 A\n",
      "he LORD followed them.\n",
      "\n",
      "6:9 And the armed men went\n",
      "shew thyself pure; and with the froward\n",
      "thou wilt \n",
      "hey were destroyed before the LORD, and before his\n",
      "ter than an ox or bullock\n",
      "that hath horns and hoof\n",
      "th from polluting it, and taketh hold\n",
      "of my covena\n",
      " saith the Lord GOD, be it known\n",
      "unto you: be asha\n",
      "23 If any man have ears to hear, let him hear.\n",
      "\n",
      "4:\n",
      "t after the most\n",
      "straitest sect of our religion I \n"
     ]
    }
   ],
   "source": [
    "for k in y:\n",
    "  text = ''\n",
    "  for i in k:\n",
    "    text = text + int2char[i]\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hxj07FAYZbV"
   },
   "source": [
    "# Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wn1Y2qEWQlD"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps = 100, n_hidden = 256, n_layers = 2, drop_prob = 0.5, lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x, (h,c) = self.lstm(x,hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, (h,c)\n",
    "\n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "\n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "\n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encoder(x, len(self.chars))\n",
    "\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if cuda:\n",
    "            inputs.cuda()\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)    \n",
    "        p = F.softmax(out, dim=1).data\n",
    "\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p = p/p.sum())\n",
    "\n",
    "        return self.int2char[char], h \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.bias.data.uniform_(-1,1)\n",
    "\n",
    "    def init_hidden(self,n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "        weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crPs0aaWltgK"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LI6T5B8Olszd"
   },
   "outputs": [],
   "source": [
    "def train(net,data,epochs=10,n_seqs=10,n_steps=50,lr=0.001,clip=5,val_frac=0.1,cuda=False,print_every=10):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(encoded, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "\n",
    "        x = one_hot_encoder(x,n_chars)\n",
    "        inputs,targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "          \n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        output, h = net.forward(inputs, h)\n",
    "        loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        if counter % print_every == 0:\n",
    "              \n",
    "            val_h = net.init_hidden(n_seqs)\n",
    "            val_losses = []\n",
    "              \n",
    "            for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                  \n",
    "                x = one_hot_encoder(x, n_chars)\n",
    "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, targets = x, y\n",
    "                if cuda:\n",
    "                  inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                output, val_h = net.forward(inputs, val_h)\n",
    "                val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "              \n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "            \"Step: {}...\".format(counter),\n",
    "            \"Loss: {:.4f}...\".format(loss.item()),\n",
    "            \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "qHXio0Lqi83C",
    "outputId": "6029a996-d13d-496c-d1d9-a67f7eaab762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(80, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "\n",
    "net = CharRNN(chars,n_hidden=512, n_layers=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "XF5zwi2fmAf9",
    "outputId": "83150368-9ed4-423a-d2f5-25650f18e491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2596... Val Loss: 3.2639\n",
      "Epoch: 1/20... Step: 20... Loss: 3.2092... Val Loss: 3.2032\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1754... Val Loss: 3.1812\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1689... Val Loss: 3.1728\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1672... Val Loss: 3.1688\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1600... Val Loss: 3.1635\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1574... Val Loss: 3.1605\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1619... Val Loss: 3.1497\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1172... Val Loss: 3.1265\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0670... Val Loss: 3.0681\n",
      "Epoch: 1/20... Step: 110... Loss: 2.9509... Val Loss: 2.9676\n",
      "Epoch: 1/20... Step: 120... Loss: 3.3459... Val Loss: 2.9048\n",
      "Epoch: 1/20... Step: 130... Loss: 2.8346... Val Loss: 2.8600\n",
      "Epoch: 1/20... Step: 140... Loss: 2.7227... Val Loss: 2.7544\n",
      "Epoch: 1/20... Step: 150... Loss: 2.6179... Val Loss: 2.6781\n",
      "Epoch: 1/20... Step: 160... Loss: 2.5405... Val Loss: 2.5906\n",
      "Epoch: 1/20... Step: 170... Loss: 2.5674... Val Loss: 2.5035\n",
      "Epoch: 1/20... Step: 180... Loss: 2.3739... Val Loss: 2.4521\n",
      "Epoch: 1/20... Step: 190... Loss: 2.3176... Val Loss: 2.3974\n",
      "Epoch: 1/20... Step: 200... Loss: 2.2655... Val Loss: 2.3608\n",
      "Epoch: 1/20... Step: 210... Loss: 2.2577... Val Loss: 2.3348\n",
      "Epoch: 1/20... Step: 220... Loss: 2.1665... Val Loss: 2.3141\n",
      "Epoch: 1/20... Step: 230... Loss: 2.2058... Val Loss: 2.2937\n",
      "Epoch: 1/20... Step: 240... Loss: 2.1685... Val Loss: 2.2712\n",
      "Epoch: 1/20... Step: 250... Loss: 2.1451... Val Loss: 2.2513\n",
      "Epoch: 1/20... Step: 260... Loss: 2.1347... Val Loss: 2.2292\n",
      "Epoch: 1/20... Step: 270... Loss: 2.0920... Val Loss: 2.2107\n",
      "Epoch: 1/20... Step: 280... Loss: 2.1036... Val Loss: 2.2009\n",
      "Epoch: 1/20... Step: 290... Loss: 2.0917... Val Loss: 2.1754\n",
      "Epoch: 1/20... Step: 300... Loss: 2.0342... Val Loss: 2.1624\n",
      "Epoch: 1/20... Step: 310... Loss: 2.0463... Val Loss: 2.1530\n",
      "Epoch: 1/20... Step: 320... Loss: 2.0338... Val Loss: 2.1370\n",
      "Epoch: 1/20... Step: 330... Loss: 2.0118... Val Loss: 2.1237\n",
      "Epoch: 2/20... Step: 340... Loss: 2.0875... Val Loss: 2.1054\n",
      "Epoch: 2/20... Step: 350... Loss: 1.9839... Val Loss: 2.0905\n",
      "Epoch: 2/20... Step: 360... Loss: 1.9848... Val Loss: 2.0888\n",
      "Epoch: 2/20... Step: 370... Loss: 1.9495... Val Loss: 2.0682\n",
      "Epoch: 2/20... Step: 380... Loss: 1.9278... Val Loss: 2.0456\n",
      "Epoch: 2/20... Step: 390... Loss: 1.9177... Val Loss: 2.0314\n",
      "Epoch: 2/20... Step: 400... Loss: 1.8876... Val Loss: 2.0224\n",
      "Epoch: 2/20... Step: 410... Loss: 1.9242... Val Loss: 2.0191\n",
      "Epoch: 2/20... Step: 420... Loss: 1.9181... Val Loss: 2.0136\n",
      "Epoch: 2/20... Step: 430... Loss: 1.8590... Val Loss: 1.9984\n",
      "Epoch: 2/20... Step: 440... Loss: 1.8479... Val Loss: 1.9949\n",
      "Epoch: 2/20... Step: 450... Loss: 1.8396... Val Loss: 1.9800\n",
      "Epoch: 2/20... Step: 460... Loss: 1.8682... Val Loss: 1.9674\n",
      "Epoch: 2/20... Step: 470... Loss: 1.8175... Val Loss: 1.9578\n",
      "Epoch: 2/20... Step: 480... Loss: 1.8234... Val Loss: 1.9539\n",
      "Epoch: 2/20... Step: 490... Loss: 1.7956... Val Loss: 1.9381\n",
      "Epoch: 2/20... Step: 500... Loss: 1.8268... Val Loss: 1.9338\n",
      "Epoch: 2/20... Step: 510... Loss: 1.8022... Val Loss: 1.9272\n",
      "Epoch: 2/20... Step: 520... Loss: 1.7865... Val Loss: 1.9133\n",
      "Epoch: 2/20... Step: 530... Loss: 1.8145... Val Loss: 1.9087\n",
      "Epoch: 2/20... Step: 540... Loss: 1.7523... Val Loss: 1.9083\n",
      "Epoch: 2/20... Step: 550... Loss: 1.7765... Val Loss: 1.8982\n",
      "Epoch: 2/20... Step: 560... Loss: 1.7462... Val Loss: 1.8876\n",
      "Epoch: 2/20... Step: 570... Loss: 1.7459... Val Loss: 1.8792\n",
      "Epoch: 2/20... Step: 580... Loss: 1.7197... Val Loss: 1.8786\n",
      "Epoch: 2/20... Step: 590... Loss: 1.7441... Val Loss: 1.8688\n",
      "Epoch: 2/20... Step: 600... Loss: 1.7405... Val Loss: 1.8627\n",
      "Epoch: 2/20... Step: 610... Loss: 1.7298... Val Loss: 1.8560\n",
      "Epoch: 2/20... Step: 620... Loss: 1.7358... Val Loss: 1.8539\n",
      "Epoch: 2/20... Step: 630... Loss: 1.7124... Val Loss: 1.8471\n",
      "Epoch: 2/20... Step: 640... Loss: 1.7208... Val Loss: 1.8429\n",
      "Epoch: 2/20... Step: 650... Loss: 1.6861... Val Loss: 1.8347\n",
      "Epoch: 2/20... Step: 660... Loss: 1.6817... Val Loss: 1.8246\n",
      "Epoch: 2/20... Step: 670... Loss: 1.6748... Val Loss: 1.8272\n",
      "Epoch: 3/20... Step: 680... Loss: 1.6701... Val Loss: 1.8184\n",
      "Epoch: 3/20... Step: 690... Loss: 1.6612... Val Loss: 1.8061\n",
      "Epoch: 3/20... Step: 700... Loss: 1.6591... Val Loss: 1.8027\n",
      "Epoch: 3/20... Step: 710... Loss: 1.6802... Val Loss: 1.8030\n",
      "Epoch: 3/20... Step: 720... Loss: 1.6432... Val Loss: 1.7926\n",
      "Epoch: 3/20... Step: 730... Loss: 1.6621... Val Loss: 1.7886\n",
      "Epoch: 3/20... Step: 740... Loss: 1.6732... Val Loss: 1.7862\n",
      "Epoch: 3/20... Step: 750... Loss: 1.6401... Val Loss: 1.7878\n",
      "Epoch: 3/20... Step: 760... Loss: 1.6198... Val Loss: 1.7773\n",
      "Epoch: 3/20... Step: 770... Loss: 1.6384... Val Loss: 1.7774\n",
      "Epoch: 3/20... Step: 780... Loss: 1.6375... Val Loss: 1.7713\n",
      "Epoch: 3/20... Step: 790... Loss: 1.6325... Val Loss: 1.7670\n",
      "Epoch: 3/20... Step: 800... Loss: 1.6305... Val Loss: 1.7635\n",
      "Epoch: 3/20... Step: 810... Loss: 1.6125... Val Loss: 1.7593\n",
      "Epoch: 3/20... Step: 820... Loss: 1.6062... Val Loss: 1.7544\n",
      "Epoch: 3/20... Step: 830... Loss: 1.6255... Val Loss: 1.7498\n",
      "Epoch: 3/20... Step: 840... Loss: 1.6170... Val Loss: 1.7513\n",
      "Epoch: 3/20... Step: 850... Loss: 1.6143... Val Loss: 1.7528\n",
      "Epoch: 3/20... Step: 860... Loss: 1.6182... Val Loss: 1.7408\n",
      "Epoch: 3/20... Step: 870... Loss: 1.6104... Val Loss: 1.7397\n",
      "Epoch: 3/20... Step: 880... Loss: 1.5921... Val Loss: 1.7435\n",
      "Epoch: 3/20... Step: 890... Loss: 1.6129... Val Loss: 1.7352\n",
      "Epoch: 3/20... Step: 900... Loss: 1.6068... Val Loss: 1.7339\n",
      "Epoch: 3/20... Step: 910... Loss: 1.5723... Val Loss: 1.7268\n",
      "Epoch: 3/20... Step: 920... Loss: 1.5850... Val Loss: 1.7234\n",
      "Epoch: 3/20... Step: 930... Loss: 1.5603... Val Loss: 1.7165\n",
      "Epoch: 3/20... Step: 940... Loss: 1.5605... Val Loss: 1.7194\n",
      "Epoch: 3/20... Step: 950... Loss: 1.5647... Val Loss: 1.7146\n",
      "Epoch: 3/20... Step: 960... Loss: 1.5819... Val Loss: 1.7144\n",
      "Epoch: 3/20... Step: 970... Loss: 1.5842... Val Loss: 1.7051\n",
      "Epoch: 3/20... Step: 980... Loss: 1.5705... Val Loss: 1.7071\n",
      "Epoch: 3/20... Step: 990... Loss: 1.5561... Val Loss: 1.7108\n",
      "Epoch: 3/20... Step: 1000... Loss: 1.5565... Val Loss: 1.7018\n",
      "Epoch: 3/20... Step: 1010... Loss: 1.5081... Val Loss: 1.6999\n",
      "Epoch: 4/20... Step: 1020... Loss: 1.5377... Val Loss: 1.6936\n",
      "Epoch: 4/20... Step: 1030... Loss: 1.5558... Val Loss: 1.6963\n",
      "Epoch: 4/20... Step: 1040... Loss: 1.5593... Val Loss: 1.6815\n",
      "Epoch: 4/20... Step: 1050... Loss: 1.5209... Val Loss: 1.6895\n",
      "Epoch: 4/20... Step: 1060... Loss: 1.5150... Val Loss: 1.6871\n",
      "Epoch: 4/20... Step: 1070... Loss: 1.5411... Val Loss: 1.6820\n",
      "Epoch: 4/20... Step: 1080... Loss: 1.5773... Val Loss: 1.6814\n",
      "Epoch: 4/20... Step: 1090... Loss: 1.4988... Val Loss: 1.6809\n",
      "Epoch: 4/20... Step: 1100... Loss: 1.5278... Val Loss: 1.6839\n",
      "Epoch: 4/20... Step: 1110... Loss: 1.5040... Val Loss: 1.6751\n",
      "Epoch: 4/20... Step: 1120... Loss: 1.5059... Val Loss: 1.6718\n",
      "Epoch: 4/20... Step: 1130... Loss: 1.5237... Val Loss: 1.6747\n",
      "Epoch: 4/20... Step: 1140... Loss: 1.4912... Val Loss: 1.6735\n",
      "Epoch: 4/20... Step: 1150... Loss: 1.5270... Val Loss: 1.6686\n",
      "Epoch: 4/20... Step: 1160... Loss: 1.4818... Val Loss: 1.6646\n",
      "Epoch: 4/20... Step: 1170... Loss: 1.5345... Val Loss: 1.6636\n",
      "Epoch: 4/20... Step: 1180... Loss: 1.5137... Val Loss: 1.6663\n",
      "Epoch: 4/20... Step: 1190... Loss: 1.5209... Val Loss: 1.6632\n",
      "Epoch: 4/20... Step: 1200... Loss: 1.5069... Val Loss: 1.6551\n",
      "Epoch: 4/20... Step: 1210... Loss: 1.4976... Val Loss: 1.6560\n",
      "Epoch: 4/20... Step: 1220... Loss: 1.5168... Val Loss: 1.6515\n",
      "Epoch: 4/20... Step: 1230... Loss: 1.5000... Val Loss: 1.6536\n",
      "Epoch: 4/20... Step: 1240... Loss: 1.4767... Val Loss: 1.6507\n",
      "Epoch: 4/20... Step: 1250... Loss: 1.5149... Val Loss: 1.6514\n",
      "Epoch: 4/20... Step: 1260... Loss: 1.4949... Val Loss: 1.6508\n",
      "Epoch: 4/20... Step: 1270... Loss: 1.5025... Val Loss: 1.6526\n",
      "Epoch: 4/20... Step: 1280... Loss: 1.4809... Val Loss: 1.6462\n",
      "Epoch: 4/20... Step: 1290... Loss: 1.4797... Val Loss: 1.6415\n",
      "Epoch: 4/20... Step: 1300... Loss: 1.4986... Val Loss: 1.6410\n",
      "Epoch: 4/20... Step: 1310... Loss: 1.4956... Val Loss: 1.6390\n",
      "Epoch: 4/20... Step: 1320... Loss: 1.4946... Val Loss: 1.6412\n",
      "Epoch: 4/20... Step: 1330... Loss: 1.4567... Val Loss: 1.6418\n",
      "Epoch: 4/20... Step: 1340... Loss: 1.4849... Val Loss: 1.6371\n",
      "Epoch: 4/20... Step: 1350... Loss: 1.4832... Val Loss: 1.6318\n",
      "Epoch: 5/20... Step: 1360... Loss: 1.4433... Val Loss: 1.6322\n",
      "Epoch: 5/20... Step: 1370... Loss: 1.4982... Val Loss: 1.6365\n",
      "Epoch: 5/20... Step: 1380... Loss: 1.4778... Val Loss: 1.6228\n",
      "Epoch: 5/20... Step: 1390... Loss: 1.4698... Val Loss: 1.6229\n",
      "Epoch: 5/20... Step: 1400... Loss: 1.4733... Val Loss: 1.6290\n",
      "Epoch: 5/20... Step: 1410... Loss: 1.4613... Val Loss: 1.6215\n",
      "Epoch: 5/20... Step: 1420... Loss: 1.4745... Val Loss: 1.6226\n",
      "Epoch: 5/20... Step: 1430... Loss: 1.4703... Val Loss: 1.6188\n",
      "Epoch: 5/20... Step: 1440... Loss: 1.4512... Val Loss: 1.6249\n",
      "Epoch: 5/20... Step: 1450... Loss: 1.4396... Val Loss: 1.6196\n",
      "Epoch: 5/20... Step: 1460... Loss: 1.4373... Val Loss: 1.6183\n",
      "Epoch: 5/20... Step: 1470... Loss: 1.4305... Val Loss: 1.6173\n",
      "Epoch: 5/20... Step: 1480... Loss: 1.4619... Val Loss: 1.6164\n",
      "Epoch: 5/20... Step: 1490... Loss: 1.4654... Val Loss: 1.6090\n",
      "Epoch: 5/20... Step: 1500... Loss: 1.4356... Val Loss: 1.6129\n",
      "Epoch: 5/20... Step: 1510... Loss: 1.4646... Val Loss: 1.6187\n",
      "Epoch: 5/20... Step: 1520... Loss: 1.4829... Val Loss: 1.6158\n",
      "Epoch: 5/20... Step: 1530... Loss: 1.4688... Val Loss: 1.6159\n",
      "Epoch: 5/20... Step: 1540... Loss: 1.4613... Val Loss: 1.6108\n",
      "Epoch: 5/20... Step: 1550... Loss: 1.4684... Val Loss: 1.6073\n",
      "Epoch: 5/20... Step: 1560... Loss: 1.4437... Val Loss: 1.6059\n",
      "Epoch: 5/20... Step: 1570... Loss: 1.4457... Val Loss: 1.6095\n",
      "Epoch: 5/20... Step: 1580... Loss: 1.4545... Val Loss: 1.6052\n",
      "Epoch: 5/20... Step: 1590... Loss: 1.4520... Val Loss: 1.6057\n",
      "Epoch: 5/20... Step: 1600... Loss: 1.4634... Val Loss: 1.6057\n",
      "Epoch: 5/20... Step: 1610... Loss: 1.4666... Val Loss: 1.6080\n",
      "Epoch: 5/20... Step: 1620... Loss: 1.4339... Val Loss: 1.6024\n",
      "Epoch: 5/20... Step: 1630... Loss: 1.4421... Val Loss: 1.5975\n",
      "Epoch: 5/20... Step: 1640... Loss: 1.4556... Val Loss: 1.5967\n",
      "Epoch: 5/20... Step: 1650... Loss: 1.4502... Val Loss: 1.5948\n",
      "Epoch: 5/20... Step: 1660... Loss: 1.4691... Val Loss: 1.5973\n",
      "Epoch: 5/20... Step: 1670... Loss: 1.4313... Val Loss: 1.6016\n",
      "Epoch: 5/20... Step: 1680... Loss: 1.4398... Val Loss: 1.5968\n",
      "Epoch: 5/20... Step: 1690... Loss: 1.4270... Val Loss: 1.5997\n",
      "Epoch: 6/20... Step: 1700... Loss: 1.4217... Val Loss: 1.5942\n",
      "Epoch: 6/20... Step: 1710... Loss: 1.4469... Val Loss: 1.5858\n",
      "Epoch: 6/20... Step: 1720... Loss: 1.4358... Val Loss: 1.5897\n",
      "Epoch: 6/20... Step: 1730... Loss: 1.4101... Val Loss: 1.5839\n",
      "Epoch: 6/20... Step: 1740... Loss: 1.4259... Val Loss: 1.5899\n",
      "Epoch: 6/20... Step: 1750... Loss: 1.4154... Val Loss: 1.5821\n",
      "Epoch: 6/20... Step: 1760... Loss: 1.4330... Val Loss: 1.5760\n",
      "Epoch: 6/20... Step: 1770... Loss: 1.4234... Val Loss: 1.5874\n",
      "Epoch: 6/20... Step: 1780... Loss: 1.4293... Val Loss: 1.5866\n",
      "Epoch: 6/20... Step: 1790... Loss: 1.4228... Val Loss: 1.5827\n",
      "Epoch: 6/20... Step: 1800... Loss: 1.3951... Val Loss: 1.5874\n",
      "Epoch: 6/20... Step: 1810... Loss: 1.3914... Val Loss: 1.5811\n",
      "Epoch: 6/20... Step: 1820... Loss: 1.3977... Val Loss: 1.5842\n",
      "Epoch: 6/20... Step: 1830... Loss: 1.4179... Val Loss: 1.5725\n",
      "Epoch: 6/20... Step: 1840... Loss: 1.4456... Val Loss: 1.5827\n",
      "Epoch: 6/20... Step: 1850... Loss: 1.4173... Val Loss: 1.5828\n",
      "Epoch: 6/20... Step: 1860... Loss: 1.4466... Val Loss: 1.5775\n",
      "Epoch: 6/20... Step: 1870... Loss: 1.4140... Val Loss: 1.5760\n",
      "Epoch: 6/20... Step: 1880... Loss: 1.4488... Val Loss: 1.5772\n",
      "Epoch: 6/20... Step: 1890... Loss: 1.4500... Val Loss: 1.5788\n",
      "Epoch: 6/20... Step: 1900... Loss: 1.4009... Val Loss: 1.5741\n",
      "Epoch: 6/20... Step: 1910... Loss: 1.4078... Val Loss: 1.5745\n",
      "Epoch: 6/20... Step: 1920... Loss: 1.4092... Val Loss: 1.5740\n",
      "Epoch: 6/20... Step: 1930... Loss: 1.3928... Val Loss: 1.5771\n",
      "Epoch: 6/20... Step: 1940... Loss: 1.4071... Val Loss: 1.5771\n",
      "Epoch: 6/20... Step: 1950... Loss: 1.4159... Val Loss: 1.5700\n",
      "Epoch: 6/20... Step: 1960... Loss: 1.4021... Val Loss: 1.5729\n",
      "Epoch: 6/20... Step: 1970... Loss: 1.4141... Val Loss: 1.5674\n",
      "Epoch: 6/20... Step: 1980... Loss: 1.4442... Val Loss: 1.5619\n",
      "Epoch: 6/20... Step: 1990... Loss: 1.4094... Val Loss: 1.5664\n",
      "Epoch: 6/20... Step: 2000... Loss: 1.3925... Val Loss: 1.5640\n",
      "Epoch: 6/20... Step: 2010... Loss: 1.4330... Val Loss: 1.5576\n",
      "Epoch: 6/20... Step: 2020... Loss: 1.3936... Val Loss: 1.5699\n",
      "Epoch: 6/20... Step: 2030... Loss: 1.3824... Val Loss: 1.5628\n",
      "Epoch: 7/20... Step: 2040... Loss: 1.3792... Val Loss: 1.5627\n",
      "Epoch: 7/20... Step: 2050... Loss: 1.4153... Val Loss: 1.5611\n",
      "Epoch: 7/20... Step: 2060... Loss: 1.3891... Val Loss: 1.5627\n",
      "Epoch: 7/20... Step: 2070... Loss: 1.3565... Val Loss: 1.5631\n",
      "Epoch: 7/20... Step: 2080... Loss: 1.3979... Val Loss: 1.5656\n",
      "Epoch: 7/20... Step: 2090... Loss: 1.3646... Val Loss: 1.5522\n",
      "Epoch: 7/20... Step: 2100... Loss: 1.4060... Val Loss: 1.5498\n",
      "Epoch: 7/20... Step: 2110... Loss: 1.4194... Val Loss: 1.5526\n",
      "Epoch: 7/20... Step: 2120... Loss: 1.4041... Val Loss: 1.5528\n",
      "Epoch: 7/20... Step: 2130... Loss: 1.3911... Val Loss: 1.5516\n",
      "Epoch: 7/20... Step: 2140... Loss: 1.3757... Val Loss: 1.5537\n",
      "Epoch: 7/20... Step: 2150... Loss: 1.3937... Val Loss: 1.5532\n",
      "Epoch: 7/20... Step: 2160... Loss: 1.4049... Val Loss: 1.5524\n",
      "Epoch: 7/20... Step: 2170... Loss: 1.3727... Val Loss: 1.5533\n",
      "Epoch: 7/20... Step: 2180... Loss: 1.3888... Val Loss: 1.5491\n",
      "Epoch: 7/20... Step: 2190... Loss: 1.3842... Val Loss: 1.5590\n",
      "Epoch: 7/20... Step: 2200... Loss: 1.3895... Val Loss: 1.5530\n",
      "Epoch: 7/20... Step: 2210... Loss: 1.3809... Val Loss: 1.5482\n",
      "Epoch: 7/20... Step: 2220... Loss: 1.4120... Val Loss: 1.5480\n",
      "Epoch: 7/20... Step: 2230... Loss: 1.3887... Val Loss: 1.5547\n",
      "Epoch: 7/20... Step: 2240... Loss: 1.3693... Val Loss: 1.5554\n",
      "Epoch: 7/20... Step: 2250... Loss: 1.4124... Val Loss: 1.5500\n",
      "Epoch: 7/20... Step: 2260... Loss: 1.3597... Val Loss: 1.5391\n",
      "Epoch: 7/20... Step: 2270... Loss: 1.3747... Val Loss: 1.5506\n",
      "Epoch: 7/20... Step: 2280... Loss: 1.3808... Val Loss: 1.5388\n",
      "Epoch: 7/20... Step: 2290... Loss: 1.3683... Val Loss: 1.5317\n",
      "Epoch: 7/20... Step: 2300... Loss: 1.3769... Val Loss: 1.5463\n",
      "Epoch: 7/20... Step: 2310... Loss: 1.3363... Val Loss: 1.5377\n",
      "Epoch: 7/20... Step: 2320... Loss: 1.3815... Val Loss: 1.5264\n",
      "Epoch: 7/20... Step: 2330... Loss: 1.3917... Val Loss: 1.5406\n",
      "Epoch: 7/20... Step: 2340... Loss: 1.3713... Val Loss: 1.5318\n",
      "Epoch: 7/20... Step: 2350... Loss: 1.3826... Val Loss: 1.5373\n",
      "Epoch: 7/20... Step: 2360... Loss: 1.3706... Val Loss: 1.5371\n",
      "Epoch: 7/20... Step: 2370... Loss: 1.3688... Val Loss: 1.5326\n",
      "Epoch: 8/20... Step: 2380... Loss: 1.3715... Val Loss: 1.5322\n",
      "Epoch: 8/20... Step: 2390... Loss: 1.3979... Val Loss: 1.5249\n",
      "Epoch: 8/20... Step: 2400... Loss: 1.3840... Val Loss: 1.5283\n",
      "Epoch: 8/20... Step: 2410... Loss: 1.3358... Val Loss: 1.5301\n",
      "Epoch: 8/20... Step: 2420... Loss: 1.3812... Val Loss: 1.5252\n",
      "Epoch: 8/20... Step: 2430... Loss: 1.3468... Val Loss: 1.5211\n",
      "Epoch: 8/20... Step: 2440... Loss: 1.3590... Val Loss: 1.5253\n",
      "Epoch: 8/20... Step: 2450... Loss: 1.3825... Val Loss: 1.5299\n",
      "Epoch: 8/20... Step: 2460... Loss: 1.3620... Val Loss: 1.5270\n",
      "Epoch: 8/20... Step: 2470... Loss: 1.3267... Val Loss: 1.5223\n",
      "Epoch: 8/20... Step: 2480... Loss: 1.3335... Val Loss: 1.5277\n",
      "Epoch: 8/20... Step: 2490... Loss: 1.3129... Val Loss: 1.5192\n",
      "Epoch: 8/20... Step: 2500... Loss: 1.3778... Val Loss: 1.5203\n",
      "Epoch: 8/20... Step: 2510... Loss: 1.3600... Val Loss: 1.5225\n",
      "Epoch: 8/20... Step: 2520... Loss: 1.3920... Val Loss: 1.5256\n",
      "Epoch: 8/20... Step: 2530... Loss: 1.3468... Val Loss: 1.5217\n",
      "Epoch: 8/20... Step: 2540... Loss: 1.3504... Val Loss: 1.5247\n",
      "Epoch: 8/20... Step: 2550... Loss: 1.3504... Val Loss: 1.5148\n",
      "Epoch: 8/20... Step: 2560... Loss: 1.3798... Val Loss: 1.5191\n",
      "Epoch: 8/20... Step: 2570... Loss: 1.3630... Val Loss: 1.5234\n",
      "Epoch: 8/20... Step: 2580... Loss: 1.3584... Val Loss: 1.5202\n",
      "Epoch: 8/20... Step: 2590... Loss: 1.3745... Val Loss: 1.5173\n",
      "Epoch: 8/20... Step: 2600... Loss: 1.3454... Val Loss: 1.5182\n",
      "Epoch: 8/20... Step: 2610... Loss: 1.3848... Val Loss: 1.5218\n",
      "Epoch: 8/20... Step: 2620... Loss: 1.3328... Val Loss: 1.5123\n",
      "Epoch: 8/20... Step: 2630... Loss: 1.3565... Val Loss: 1.5143\n",
      "Epoch: 8/20... Step: 2640... Loss: 1.3585... Val Loss: 1.5142\n",
      "Epoch: 8/20... Step: 2650... Loss: 1.3990... Val Loss: 1.5186\n",
      "Epoch: 8/20... Step: 2660... Loss: 1.3977... Val Loss: 1.5091\n",
      "Epoch: 8/20... Step: 2670... Loss: 1.3645... Val Loss: 1.5144\n",
      "Epoch: 8/20... Step: 2680... Loss: 1.3406... Val Loss: 1.5025\n",
      "Epoch: 8/20... Step: 2690... Loss: 1.3536... Val Loss: 1.5135\n",
      "Epoch: 8/20... Step: 2700... Loss: 1.3537... Val Loss: 1.5165\n",
      "Epoch: 8/20... Step: 2710... Loss: 1.3614... Val Loss: 1.5098\n",
      "Epoch: 9/20... Step: 2720... Loss: 1.3455... Val Loss: 1.5143\n",
      "Epoch: 9/20... Step: 2730... Loss: 1.3852... Val Loss: 1.5013\n",
      "Epoch: 9/20... Step: 2740... Loss: 1.3265... Val Loss: 1.5126\n",
      "Epoch: 9/20... Step: 2750... Loss: 1.3072... Val Loss: 1.5169\n",
      "Epoch: 9/20... Step: 2760... Loss: 1.3311... Val Loss: 1.5055\n",
      "Epoch: 9/20... Step: 2770... Loss: 1.3403... Val Loss: 1.5092\n",
      "Epoch: 9/20... Step: 2780... Loss: 1.3514... Val Loss: 1.5007\n",
      "Epoch: 9/20... Step: 2790... Loss: 1.3455... Val Loss: 1.5028\n",
      "Epoch: 9/20... Step: 2800... Loss: 1.3192... Val Loss: 1.5112\n",
      "Epoch: 9/20... Step: 2810... Loss: 1.3392... Val Loss: 1.5060\n",
      "Epoch: 9/20... Step: 2820... Loss: 1.3354... Val Loss: 1.5081\n",
      "Epoch: 9/20... Step: 2830... Loss: 1.3277... Val Loss: 1.5021\n",
      "Epoch: 9/20... Step: 2840... Loss: 1.3277... Val Loss: 1.5049\n",
      "Epoch: 9/20... Step: 2850... Loss: 1.3508... Val Loss: 1.5063\n",
      "Epoch: 9/20... Step: 2860... Loss: 1.3291... Val Loss: 1.4994\n",
      "Epoch: 9/20... Step: 2870... Loss: 1.3291... Val Loss: 1.4995\n",
      "Epoch: 9/20... Step: 2880... Loss: 1.3285... Val Loss: 1.5127\n",
      "Epoch: 9/20... Step: 2890... Loss: 1.3282... Val Loss: 1.4968\n",
      "Epoch: 9/20... Step: 2900... Loss: 1.3411... Val Loss: 1.5032\n",
      "Epoch: 9/20... Step: 2910... Loss: 1.3706... Val Loss: 1.5041\n",
      "Epoch: 9/20... Step: 2920... Loss: 1.3193... Val Loss: 1.5018\n",
      "Epoch: 9/20... Step: 2930... Loss: 1.3402... Val Loss: 1.5123\n",
      "Epoch: 9/20... Step: 2940... Loss: 1.3520... Val Loss: 1.5070\n",
      "Epoch: 9/20... Step: 2950... Loss: 1.3183... Val Loss: 1.5007\n",
      "Epoch: 9/20... Step: 2960... Loss: 1.3357... Val Loss: 1.4992\n",
      "Epoch: 9/20... Step: 2970... Loss: 1.3262... Val Loss: 1.4946\n",
      "Epoch: 9/20... Step: 2980... Loss: 1.3516... Val Loss: 1.5017\n",
      "Epoch: 9/20... Step: 2990... Loss: 1.3481... Val Loss: 1.5014\n",
      "Epoch: 9/20... Step: 3000... Loss: 1.3539... Val Loss: 1.4958\n",
      "Epoch: 9/20... Step: 3010... Loss: 1.3483... Val Loss: 1.4975\n",
      "Epoch: 9/20... Step: 3020... Loss: 1.3082... Val Loss: 1.4919\n",
      "Epoch: 9/20... Step: 3030... Loss: 1.3278... Val Loss: 1.4962\n",
      "Epoch: 9/20... Step: 3040... Loss: 1.3383... Val Loss: 1.4909\n",
      "Epoch: 9/20... Step: 3050... Loss: 1.3321... Val Loss: 1.4966\n",
      "Epoch: 10/20... Step: 3060... Loss: 1.3267... Val Loss: 1.4899\n",
      "Epoch: 10/20... Step: 3070... Loss: 1.3705... Val Loss: 1.4930\n",
      "Epoch: 10/20... Step: 3080... Loss: 1.3334... Val Loss: 1.4969\n",
      "Epoch: 10/20... Step: 3090... Loss: 1.2768... Val Loss: 1.4942\n",
      "Epoch: 10/20... Step: 3100... Loss: 1.3354... Val Loss: 1.4897\n",
      "Epoch: 10/20... Step: 3110... Loss: 1.3042... Val Loss: 1.4934\n",
      "Epoch: 10/20... Step: 3120... Loss: 1.3312... Val Loss: 1.4982\n",
      "Epoch: 10/20... Step: 3130... Loss: 1.3399... Val Loss: 1.4921\n",
      "Epoch: 10/20... Step: 3140... Loss: 1.3073... Val Loss: 1.4964\n",
      "Epoch: 10/20... Step: 3150... Loss: 1.2852... Val Loss: 1.4887\n",
      "Epoch: 10/20... Step: 3160... Loss: 1.3112... Val Loss: 1.4970\n",
      "Epoch: 10/20... Step: 3170... Loss: 1.3207... Val Loss: 1.4981\n",
      "Epoch: 10/20... Step: 3180... Loss: 1.3280... Val Loss: 1.4936\n",
      "Epoch: 10/20... Step: 3190... Loss: 1.3207... Val Loss: 1.4874\n",
      "Epoch: 10/20... Step: 3200... Loss: 1.3425... Val Loss: 1.4893\n",
      "Epoch: 10/20... Step: 3210... Loss: 1.3389... Val Loss: 1.4923\n",
      "Epoch: 10/20... Step: 3220... Loss: 1.3193... Val Loss: 1.4890\n",
      "Epoch: 10/20... Step: 3230... Loss: 1.3250... Val Loss: 1.4939\n",
      "Epoch: 10/20... Step: 3240... Loss: 1.3224... Val Loss: 1.4873\n",
      "Epoch: 10/20... Step: 3250... Loss: 1.3345... Val Loss: 1.4918\n",
      "Epoch: 10/20... Step: 3260... Loss: 1.3525... Val Loss: 1.4849\n",
      "Epoch: 10/20... Step: 3270... Loss: 1.3467... Val Loss: 1.4905\n",
      "Epoch: 10/20... Step: 3280... Loss: 1.3203... Val Loss: 1.4911\n",
      "Epoch: 10/20... Step: 3290... Loss: 1.3367... Val Loss: 1.4914\n",
      "Epoch: 10/20... Step: 3300... Loss: 1.3077... Val Loss: 1.4825\n",
      "Epoch: 10/20... Step: 3310... Loss: 1.3218... Val Loss: 1.4902\n",
      "Epoch: 10/20... Step: 3320... Loss: 1.3014... Val Loss: 1.4915\n",
      "Epoch: 10/20... Step: 3330... Loss: 1.3288... Val Loss: 1.4866\n",
      "Epoch: 10/20... Step: 3340... Loss: 1.3521... Val Loss: 1.4927\n",
      "Epoch: 10/20... Step: 3350... Loss: 1.3265... Val Loss: 1.4837\n",
      "Epoch: 10/20... Step: 3360... Loss: 1.2845... Val Loss: 1.4802\n",
      "Epoch: 10/20... Step: 3370... Loss: 1.3169... Val Loss: 1.4872\n",
      "Epoch: 10/20... Step: 3380... Loss: 1.3226... Val Loss: 1.4791\n",
      "Epoch: 10/20... Step: 3390... Loss: 1.3894... Val Loss: 1.4903\n",
      "Epoch: 11/20... Step: 3400... Loss: 1.3224... Val Loss: 1.4821\n",
      "Epoch: 11/20... Step: 3410... Loss: 1.3094... Val Loss: 1.4856\n",
      "Epoch: 11/20... Step: 3420... Loss: 1.3094... Val Loss: 1.4849\n",
      "Epoch: 11/20... Step: 3430... Loss: 1.3068... Val Loss: 1.4877\n",
      "Epoch: 11/20... Step: 3440... Loss: 1.2889... Val Loss: 1.4783\n",
      "Epoch: 11/20... Step: 3450... Loss: 1.3112... Val Loss: 1.4746\n",
      "Epoch: 11/20... Step: 3460... Loss: 1.3179... Val Loss: 1.4858\n",
      "Epoch: 11/20... Step: 3470... Loss: 1.3296... Val Loss: 1.4807\n",
      "Epoch: 11/20... Step: 3480... Loss: 1.3043... Val Loss: 1.4905\n",
      "Epoch: 11/20... Step: 3490... Loss: 1.3218... Val Loss: 1.4799\n",
      "Epoch: 11/20... Step: 3500... Loss: 1.2893... Val Loss: 1.4801\n",
      "Epoch: 11/20... Step: 3510... Loss: 1.3191... Val Loss: 1.4773\n",
      "Epoch: 11/20... Step: 3520... Loss: 1.2905... Val Loss: 1.4827\n",
      "Epoch: 11/20... Step: 3530... Loss: 1.2788... Val Loss: 1.4781\n",
      "Epoch: 11/20... Step: 3540... Loss: 1.3292... Val Loss: 1.4838\n",
      "Epoch: 11/20... Step: 3550... Loss: 1.3081... Val Loss: 1.4868\n",
      "Epoch: 11/20... Step: 3560... Loss: 1.3412... Val Loss: 1.4818\n",
      "Epoch: 11/20... Step: 3570... Loss: 1.3237... Val Loss: 1.4788\n",
      "Epoch: 11/20... Step: 3580... Loss: 1.3137... Val Loss: 1.4803\n",
      "Epoch: 11/20... Step: 3590... Loss: 1.3293... Val Loss: 1.4766\n",
      "Epoch: 11/20... Step: 3600... Loss: 1.3066... Val Loss: 1.4795\n",
      "Epoch: 11/20... Step: 3610... Loss: 1.3115... Val Loss: 1.4752\n",
      "Epoch: 11/20... Step: 3620... Loss: 1.3077... Val Loss: 1.4796\n",
      "Epoch: 11/20... Step: 3630... Loss: 1.3143... Val Loss: 1.4797\n",
      "Epoch: 11/20... Step: 3640... Loss: 1.3049... Val Loss: 1.4765\n",
      "Epoch: 11/20... Step: 3650... Loss: 1.2998... Val Loss: 1.4818\n",
      "Epoch: 11/20... Step: 3660... Loss: 1.2883... Val Loss: 1.4749\n",
      "Epoch: 11/20... Step: 3670... Loss: 1.3184... Val Loss: 1.4742\n",
      "Epoch: 11/20... Step: 3680... Loss: 1.3234... Val Loss: 1.4725\n",
      "Epoch: 11/20... Step: 3690... Loss: 1.2858... Val Loss: 1.4825\n",
      "Epoch: 11/20... Step: 3700... Loss: 1.3067... Val Loss: 1.4660\n",
      "Epoch: 11/20... Step: 3710... Loss: 1.3114... Val Loss: 1.4700\n",
      "Epoch: 11/20... Step: 3720... Loss: 1.3112... Val Loss: 1.4665\n",
      "Epoch: 12/20... Step: 3730... Loss: 1.4858... Val Loss: 1.4948\n",
      "Epoch: 12/20... Step: 3740... Loss: 1.3113... Val Loss: 1.4705\n",
      "Epoch: 12/20... Step: 3750... Loss: 1.3310... Val Loss: 1.4729\n",
      "Epoch: 12/20... Step: 3760... Loss: 1.3031... Val Loss: 1.4780\n",
      "Epoch: 12/20... Step: 3770... Loss: 1.2985... Val Loss: 1.4664\n",
      "Epoch: 12/20... Step: 3780... Loss: 1.2759... Val Loss: 1.4646\n",
      "Epoch: 12/20... Step: 3790... Loss: 1.2798... Val Loss: 1.4691\n",
      "Epoch: 12/20... Step: 3800... Loss: 1.3175... Val Loss: 1.4701\n",
      "Epoch: 12/20... Step: 3810... Loss: 1.3081... Val Loss: 1.4710\n",
      "Epoch: 12/20... Step: 3820... Loss: 1.2855... Val Loss: 1.4705\n",
      "Epoch: 12/20... Step: 3830... Loss: 1.2641... Val Loss: 1.4745\n",
      "Epoch: 12/20... Step: 3840... Loss: 1.2931... Val Loss: 1.4766\n",
      "Epoch: 12/20... Step: 3850... Loss: 1.3055... Val Loss: 1.4688\n",
      "Epoch: 12/20... Step: 3860... Loss: 1.2873... Val Loss: 1.4714\n",
      "Epoch: 12/20... Step: 3870... Loss: 1.2848... Val Loss: 1.4659\n",
      "Epoch: 12/20... Step: 3880... Loss: 1.2824... Val Loss: 1.4697\n",
      "Epoch: 12/20... Step: 3890... Loss: 1.3329... Val Loss: 1.4739\n",
      "Epoch: 12/20... Step: 3900... Loss: 1.3032... Val Loss: 1.4704\n",
      "Epoch: 12/20... Step: 3910... Loss: 1.2948... Val Loss: 1.4693\n",
      "Epoch: 12/20... Step: 3920... Loss: 1.3531... Val Loss: 1.4679\n",
      "Epoch: 12/20... Step: 3930... Loss: 1.2944... Val Loss: 1.4690\n",
      "Epoch: 12/20... Step: 3940... Loss: 1.3023... Val Loss: 1.4719\n",
      "Epoch: 12/20... Step: 3950... Loss: 1.2887... Val Loss: 1.4670\n",
      "Epoch: 12/20... Step: 3960... Loss: 1.2945... Val Loss: 1.4690\n",
      "Epoch: 12/20... Step: 3970... Loss: 1.2788... Val Loss: 1.4678\n",
      "Epoch: 12/20... Step: 3980... Loss: 1.3109... Val Loss: 1.4744\n",
      "Epoch: 12/20... Step: 3990... Loss: 1.3248... Val Loss: 1.4700\n",
      "Epoch: 12/20... Step: 4000... Loss: 1.3126... Val Loss: 1.4701\n",
      "Epoch: 12/20... Step: 4010... Loss: 1.3292... Val Loss: 1.4792\n",
      "Epoch: 12/20... Step: 4020... Loss: 1.3168... Val Loss: 1.4697\n",
      "Epoch: 12/20... Step: 4030... Loss: 1.3119... Val Loss: 1.4758\n",
      "Epoch: 12/20... Step: 4040... Loss: 1.2865... Val Loss: 1.4681\n",
      "Epoch: 12/20... Step: 4050... Loss: 1.2748... Val Loss: 1.4597\n",
      "Epoch: 12/20... Step: 4060... Loss: 1.3002... Val Loss: 1.4597\n",
      "Epoch: 13/20... Step: 4070... Loss: 1.3001... Val Loss: 1.4893\n",
      "Epoch: 13/20... Step: 4080... Loss: 1.2934... Val Loss: 1.4637\n",
      "Epoch: 13/20... Step: 4090... Loss: 1.3106... Val Loss: 1.4639\n",
      "Epoch: 13/20... Step: 4100... Loss: 1.3066... Val Loss: 1.4701\n",
      "Epoch: 13/20... Step: 4110... Loss: 1.2999... Val Loss: 1.4644\n",
      "Epoch: 13/20... Step: 4120... Loss: 1.2994... Val Loss: 1.4601\n",
      "Epoch: 13/20... Step: 4130... Loss: 1.3154... Val Loss: 1.4654\n",
      "Epoch: 13/20... Step: 4140... Loss: 1.2822... Val Loss: 1.4603\n",
      "Epoch: 13/20... Step: 4150... Loss: 1.2726... Val Loss: 1.4585\n",
      "Epoch: 13/20... Step: 4160... Loss: 1.3113... Val Loss: 1.4643\n",
      "Epoch: 13/20... Step: 4170... Loss: 1.3158... Val Loss: 1.4666\n",
      "Epoch: 13/20... Step: 4180... Loss: 1.2943... Val Loss: 1.4576\n",
      "Epoch: 13/20... Step: 4190... Loss: 1.2999... Val Loss: 1.4709\n",
      "Epoch: 13/20... Step: 4200... Loss: 1.2954... Val Loss: 1.4661\n",
      "Epoch: 13/20... Step: 4210... Loss: 1.2972... Val Loss: 1.4577\n",
      "Epoch: 13/20... Step: 4220... Loss: 1.2921... Val Loss: 1.4646\n",
      "Epoch: 13/20... Step: 4230... Loss: 1.3084... Val Loss: 1.4636\n",
      "Epoch: 13/20... Step: 4240... Loss: 1.3113... Val Loss: 1.4634\n",
      "Epoch: 13/20... Step: 4250... Loss: 1.2901... Val Loss: 1.4567\n",
      "Epoch: 13/20... Step: 4260... Loss: 1.3045... Val Loss: 1.4612\n",
      "Epoch: 13/20... Step: 4270... Loss: 1.3167... Val Loss: 1.4647\n",
      "Epoch: 13/20... Step: 4280... Loss: 1.3218... Val Loss: 1.4587\n",
      "Epoch: 13/20... Step: 4290... Loss: 1.3028... Val Loss: 1.4633\n",
      "Epoch: 13/20... Step: 4300... Loss: 1.2888... Val Loss: 1.4594\n",
      "Epoch: 13/20... Step: 4310... Loss: 1.2951... Val Loss: 1.4581\n",
      "Epoch: 13/20... Step: 4320... Loss: 1.2794... Val Loss: 1.4639\n",
      "Epoch: 13/20... Step: 4330... Loss: 1.2860... Val Loss: 1.4584\n",
      "Epoch: 13/20... Step: 4340... Loss: 1.2803... Val Loss: 1.4589\n",
      "Epoch: 13/20... Step: 4350... Loss: 1.3196... Val Loss: 1.4625\n",
      "Epoch: 13/20... Step: 4360... Loss: 1.3119... Val Loss: 1.4579\n",
      "Epoch: 13/20... Step: 4370... Loss: 1.3092... Val Loss: 1.4608\n",
      "Epoch: 13/20... Step: 4380... Loss: 1.3110... Val Loss: 1.4580\n",
      "Epoch: 13/20... Step: 4390... Loss: 1.3059... Val Loss: 1.4548\n",
      "Epoch: 13/20... Step: 4400... Loss: 1.2533... Val Loss: 1.4561\n",
      "Epoch: 14/20... Step: 4410... Loss: 1.2937... Val Loss: 1.4744\n",
      "Epoch: 14/20... Step: 4420... Loss: 1.3080... Val Loss: 1.4562\n",
      "Epoch: 14/20... Step: 4430... Loss: 1.3022... Val Loss: 1.4533\n",
      "Epoch: 14/20... Step: 4440... Loss: 1.2812... Val Loss: 1.4631\n",
      "Epoch: 14/20... Step: 4450... Loss: 1.2746... Val Loss: 1.4567\n",
      "Epoch: 14/20... Step: 4460... Loss: 1.2944... Val Loss: 1.4557\n",
      "Epoch: 14/20... Step: 4470... Loss: 1.3362... Val Loss: 1.4596\n",
      "Epoch: 14/20... Step: 4480... Loss: 1.2562... Val Loss: 1.4492\n",
      "Epoch: 14/20... Step: 4490... Loss: 1.2713... Val Loss: 1.4539\n",
      "Epoch: 14/20... Step: 4500... Loss: 1.2601... Val Loss: 1.4536\n",
      "Epoch: 14/20... Step: 4510... Loss: 1.2824... Val Loss: 1.4581\n",
      "Epoch: 14/20... Step: 4520... Loss: 1.2915... Val Loss: 1.4588\n",
      "Epoch: 14/20... Step: 4530... Loss: 1.2695... Val Loss: 1.4553\n",
      "Epoch: 14/20... Step: 4540... Loss: 1.2962... Val Loss: 1.4529\n",
      "Epoch: 14/20... Step: 4550... Loss: 1.2605... Val Loss: 1.4605\n",
      "Epoch: 14/20... Step: 4560... Loss: 1.3149... Val Loss: 1.4560\n",
      "Epoch: 14/20... Step: 4570... Loss: 1.3005... Val Loss: 1.4627\n",
      "Epoch: 14/20... Step: 4580... Loss: 1.2955... Val Loss: 1.4527\n",
      "Epoch: 14/20... Step: 4590... Loss: 1.2862... Val Loss: 1.4520\n",
      "Epoch: 14/20... Step: 4600... Loss: 1.2896... Val Loss: 1.4570\n",
      "Epoch: 14/20... Step: 4610... Loss: 1.3114... Val Loss: 1.4600\n",
      "Epoch: 14/20... Step: 4620... Loss: 1.2934... Val Loss: 1.4529\n",
      "Epoch: 14/20... Step: 4630... Loss: 1.2690... Val Loss: 1.4504\n",
      "Epoch: 14/20... Step: 4640... Loss: 1.3152... Val Loss: 1.4516\n",
      "Epoch: 14/20... Step: 4650... Loss: 1.2848... Val Loss: 1.4575\n",
      "Epoch: 14/20... Step: 4660... Loss: 1.2873... Val Loss: 1.4515\n",
      "Epoch: 14/20... Step: 4670... Loss: 1.2857... Val Loss: 1.4616\n",
      "Epoch: 14/20... Step: 4680... Loss: 1.2688... Val Loss: 1.4531\n",
      "Epoch: 14/20... Step: 4690... Loss: 1.3009... Val Loss: 1.4542\n",
      "Epoch: 14/20... Step: 4700... Loss: 1.2948... Val Loss: 1.4534\n",
      "Epoch: 14/20... Step: 4710... Loss: 1.2987... Val Loss: 1.4572\n",
      "Epoch: 14/20... Step: 4720... Loss: 1.2617... Val Loss: 1.4503\n",
      "Epoch: 14/20... Step: 4730... Loss: 1.2786... Val Loss: 1.4470\n",
      "Epoch: 14/20... Step: 4740... Loss: 1.2898... Val Loss: 1.4452\n",
      "Epoch: 15/20... Step: 4750... Loss: 1.2611... Val Loss: 1.4528\n",
      "Epoch: 15/20... Step: 4760... Loss: 1.3134... Val Loss: 1.4505\n",
      "Epoch: 15/20... Step: 4770... Loss: 1.2861... Val Loss: 1.4480\n",
      "Epoch: 15/20... Step: 4780... Loss: 1.2662... Val Loss: 1.4501\n",
      "Epoch: 15/20... Step: 4790... Loss: 1.2894... Val Loss: 1.4466\n",
      "Epoch: 15/20... Step: 4800... Loss: 1.2641... Val Loss: 1.4477\n",
      "Epoch: 15/20... Step: 4810... Loss: 1.2954... Val Loss: 1.4457\n",
      "Epoch: 15/20... Step: 4820... Loss: 1.2890... Val Loss: 1.4474\n",
      "Epoch: 15/20... Step: 4830... Loss: 1.2724... Val Loss: 1.4504\n",
      "Epoch: 15/20... Step: 4840... Loss: 1.2667... Val Loss: 1.4442\n",
      "Epoch: 15/20... Step: 4850... Loss: 1.2691... Val Loss: 1.4556\n",
      "Epoch: 15/20... Step: 4860... Loss: 1.2605... Val Loss: 1.4533\n",
      "Epoch: 15/20... Step: 4870... Loss: 1.2713... Val Loss: 1.4533\n",
      "Epoch: 15/20... Step: 4880... Loss: 1.2995... Val Loss: 1.4425\n",
      "Epoch: 15/20... Step: 4890... Loss: 1.2536... Val Loss: 1.4448\n",
      "Epoch: 15/20... Step: 4900... Loss: 1.2881... Val Loss: 1.4566\n",
      "Epoch: 15/20... Step: 4910... Loss: 1.3074... Val Loss: 1.4528\n",
      "Epoch: 15/20... Step: 4920... Loss: 1.2760... Val Loss: 1.4531\n",
      "Epoch: 15/20... Step: 4930... Loss: 1.3027... Val Loss: 1.4465\n",
      "Epoch: 15/20... Step: 4940... Loss: 1.2998... Val Loss: 1.4571\n",
      "Epoch: 15/20... Step: 4950... Loss: 1.2703... Val Loss: 1.4481\n",
      "Epoch: 15/20... Step: 4960... Loss: 1.2746... Val Loss: 1.4482\n",
      "Epoch: 15/20... Step: 4970... Loss: 1.2801... Val Loss: 1.4480\n",
      "Epoch: 15/20... Step: 4980... Loss: 1.2902... Val Loss: 1.4444\n",
      "Epoch: 15/20... Step: 4990... Loss: 1.3018... Val Loss: 1.4508\n",
      "Epoch: 15/20... Step: 5000... Loss: 1.2935... Val Loss: 1.4502\n",
      "Epoch: 15/20... Step: 5010... Loss: 1.2725... Val Loss: 1.4464\n",
      "Epoch: 15/20... Step: 5020... Loss: 1.2758... Val Loss: 1.4436\n",
      "Epoch: 15/20... Step: 5030... Loss: 1.2864... Val Loss: 1.4445\n",
      "Epoch: 15/20... Step: 5040... Loss: 1.2904... Val Loss: 1.4539\n",
      "Epoch: 15/20... Step: 5050... Loss: 1.3026... Val Loss: 1.4465\n",
      "Epoch: 15/20... Step: 5060... Loss: 1.2884... Val Loss: 1.4470\n",
      "Epoch: 15/20... Step: 5070... Loss: 1.2790... Val Loss: 1.4464\n",
      "Epoch: 15/20... Step: 5080... Loss: 1.2726... Val Loss: 1.4413\n",
      "Epoch: 16/20... Step: 5090... Loss: 1.2685... Val Loss: 1.4440\n",
      "Epoch: 16/20... Step: 5100... Loss: 1.2913... Val Loss: 1.4437\n",
      "Epoch: 16/20... Step: 5110... Loss: 1.2754... Val Loss: 1.4406\n",
      "Epoch: 16/20... Step: 5120... Loss: 1.2612... Val Loss: 1.4418\n",
      "Epoch: 16/20... Step: 5130... Loss: 1.2906... Val Loss: 1.4470\n",
      "Epoch: 16/20... Step: 5140... Loss: 1.2722... Val Loss: 1.4460\n",
      "Epoch: 16/20... Step: 5150... Loss: 1.2923... Val Loss: 1.4420\n",
      "Epoch: 16/20... Step: 5160... Loss: 1.2657... Val Loss: 1.4478\n",
      "Epoch: 16/20... Step: 5170... Loss: 1.2751... Val Loss: 1.4458\n",
      "Epoch: 16/20... Step: 5180... Loss: 1.2739... Val Loss: 1.4476\n",
      "Epoch: 16/20... Step: 5190... Loss: 1.2442... Val Loss: 1.4462\n",
      "Epoch: 16/20... Step: 5200... Loss: 1.2551... Val Loss: 1.4506\n",
      "Epoch: 16/20... Step: 5210... Loss: 1.2642... Val Loss: 1.4526\n",
      "Epoch: 16/20... Step: 5220... Loss: 1.2687... Val Loss: 1.4453\n",
      "Epoch: 16/20... Step: 5230... Loss: 1.2872... Val Loss: 1.4457\n",
      "Epoch: 16/20... Step: 5240... Loss: 1.2747... Val Loss: 1.4473\n",
      "Epoch: 16/20... Step: 5250... Loss: 1.3050... Val Loss: 1.4421\n",
      "Epoch: 16/20... Step: 5260... Loss: 1.2741... Val Loss: 1.4475\n",
      "Epoch: 16/20... Step: 5270... Loss: 1.2919... Val Loss: 1.4434\n",
      "Epoch: 16/20... Step: 5280... Loss: 1.3223... Val Loss: 1.4415\n",
      "Epoch: 16/20... Step: 5290... Loss: 1.2576... Val Loss: 1.4412\n",
      "Epoch: 16/20... Step: 5300... Loss: 1.2639... Val Loss: 1.4378\n",
      "Epoch: 16/20... Step: 5310... Loss: 1.2736... Val Loss: 1.4418\n",
      "Epoch: 16/20... Step: 5320... Loss: 1.2562... Val Loss: 1.4428\n",
      "Epoch: 16/20... Step: 5330... Loss: 1.2701... Val Loss: 1.4433\n",
      "Epoch: 16/20... Step: 5340... Loss: 1.2809... Val Loss: 1.4443\n",
      "Epoch: 16/20... Step: 5350... Loss: 1.2841... Val Loss: 1.4408\n",
      "Epoch: 16/20... Step: 5360... Loss: 1.2849... Val Loss: 1.4405\n",
      "Epoch: 16/20... Step: 5370... Loss: 1.3117... Val Loss: 1.4427\n",
      "Epoch: 16/20... Step: 5380... Loss: 1.2783... Val Loss: 1.4424\n",
      "Epoch: 16/20... Step: 5390... Loss: 1.2641... Val Loss: 1.4466\n",
      "Epoch: 16/20... Step: 5400... Loss: 1.2927... Val Loss: 1.4439\n",
      "Epoch: 16/20... Step: 5410... Loss: 1.2661... Val Loss: 1.4466\n",
      "Epoch: 16/20... Step: 5420... Loss: 1.2607... Val Loss: 1.4346\n",
      "Epoch: 17/20... Step: 5430... Loss: 1.2593... Val Loss: 1.4419\n",
      "Epoch: 17/20... Step: 5440... Loss: 1.2905... Val Loss: 1.4395\n",
      "Epoch: 17/20... Step: 5450... Loss: 1.2606... Val Loss: 1.4362\n",
      "Epoch: 17/20... Step: 5460... Loss: 1.2352... Val Loss: 1.4375\n",
      "Epoch: 17/20... Step: 5470... Loss: 1.2710... Val Loss: 1.4439\n",
      "Epoch: 17/20... Step: 5480... Loss: 1.2478... Val Loss: 1.4406\n",
      "Epoch: 17/20... Step: 5490... Loss: 1.2833... Val Loss: 1.4373\n",
      "Epoch: 17/20... Step: 5500... Loss: 1.3012... Val Loss: 1.4379\n",
      "Epoch: 17/20... Step: 5510... Loss: 1.2681... Val Loss: 1.4330\n",
      "Epoch: 17/20... Step: 5520... Loss: 1.2592... Val Loss: 1.4351\n",
      "Epoch: 17/20... Step: 5530... Loss: 1.2707... Val Loss: 1.4484\n",
      "Epoch: 17/20... Step: 5540... Loss: 1.2728... Val Loss: 1.4395\n",
      "Epoch: 17/20... Step: 5550... Loss: 1.2850... Val Loss: 1.4357\n",
      "Epoch: 17/20... Step: 5560... Loss: 1.2698... Val Loss: 1.4334\n",
      "Epoch: 17/20... Step: 5570... Loss: 1.2644... Val Loss: 1.4414\n",
      "Epoch: 17/20... Step: 5580... Loss: 1.2618... Val Loss: 1.4445\n",
      "Epoch: 17/20... Step: 5590... Loss: 1.2808... Val Loss: 1.4368\n",
      "Epoch: 17/20... Step: 5600... Loss: 1.2644... Val Loss: 1.4400\n",
      "Epoch: 17/20... Step: 5610... Loss: 1.2906... Val Loss: 1.4375\n",
      "Epoch: 17/20... Step: 5620... Loss: 1.2785... Val Loss: 1.4375\n",
      "Epoch: 17/20... Step: 5630... Loss: 1.2639... Val Loss: 1.4391\n",
      "Epoch: 17/20... Step: 5640... Loss: 1.2982... Val Loss: 1.4400\n",
      "Epoch: 17/20... Step: 5650... Loss: 1.2477... Val Loss: 1.4376\n",
      "Epoch: 17/20... Step: 5660... Loss: 1.2698... Val Loss: 1.4460\n",
      "Epoch: 17/20... Step: 5670... Loss: 1.2774... Val Loss: 1.4386\n",
      "Epoch: 17/20... Step: 5680... Loss: 1.2672... Val Loss: 1.4343\n",
      "Epoch: 17/20... Step: 5690... Loss: 1.2646... Val Loss: 1.4357\n",
      "Epoch: 17/20... Step: 5700... Loss: 1.2450... Val Loss: 1.4378\n",
      "Epoch: 17/20... Step: 5710... Loss: 1.2833... Val Loss: 1.4322\n",
      "Epoch: 17/20... Step: 5720... Loss: 1.2912... Val Loss: 1.4410\n",
      "Epoch: 17/20... Step: 5730... Loss: 1.2709... Val Loss: 1.4350\n",
      "Epoch: 17/20... Step: 5740... Loss: 1.2758... Val Loss: 1.4344\n",
      "Epoch: 17/20... Step: 5750... Loss: 1.2667... Val Loss: 1.4308\n",
      "Epoch: 17/20... Step: 5760... Loss: 1.2566... Val Loss: 1.4325\n",
      "Epoch: 18/20... Step: 5770... Loss: 1.2758... Val Loss: 1.4415\n",
      "Epoch: 18/20... Step: 5780... Loss: 1.2950... Val Loss: 1.4307\n",
      "Epoch: 18/20... Step: 5790... Loss: 1.2792... Val Loss: 1.4332\n",
      "Epoch: 18/20... Step: 5800... Loss: 1.2393... Val Loss: 1.4325\n",
      "Epoch: 18/20... Step: 5810... Loss: 1.2888... Val Loss: 1.4362\n",
      "Epoch: 18/20... Step: 5820... Loss: 1.2464... Val Loss: 1.4329\n",
      "Epoch: 18/20... Step: 5830... Loss: 1.2755... Val Loss: 1.4307\n",
      "Epoch: 18/20... Step: 5840... Loss: 1.2842... Val Loss: 1.4359\n",
      "Epoch: 18/20... Step: 5850... Loss: 1.2767... Val Loss: 1.4350\n",
      "Epoch: 18/20... Step: 5860... Loss: 1.2378... Val Loss: 1.4434\n",
      "Epoch: 18/20... Step: 5870... Loss: 1.2465... Val Loss: 1.4325\n",
      "Epoch: 18/20... Step: 5880... Loss: 1.2380... Val Loss: 1.4359\n",
      "Epoch: 18/20... Step: 5890... Loss: 1.2865... Val Loss: 1.4365\n",
      "Epoch: 18/20... Step: 5900... Loss: 1.2735... Val Loss: 1.4282\n",
      "Epoch: 18/20... Step: 5910... Loss: 1.3065... Val Loss: 1.4320\n",
      "Epoch: 18/20... Step: 5920... Loss: 1.2489... Val Loss: 1.4373\n",
      "Epoch: 18/20... Step: 5930... Loss: 1.2594... Val Loss: 1.4368\n",
      "Epoch: 18/20... Step: 5940... Loss: 1.2741... Val Loss: 1.4340\n",
      "Epoch: 18/20... Step: 5950... Loss: 1.2917... Val Loss: 1.4356\n",
      "Epoch: 18/20... Step: 5960... Loss: 1.2755... Val Loss: 1.4351\n",
      "Epoch: 18/20... Step: 5970... Loss: 1.2648... Val Loss: 1.4382\n",
      "Epoch: 18/20... Step: 5980... Loss: 1.2807... Val Loss: 1.4334\n",
      "Epoch: 18/20... Step: 5990... Loss: 1.2680... Val Loss: 1.4321\n",
      "Epoch: 18/20... Step: 6000... Loss: 1.2868... Val Loss: 1.4352\n",
      "Epoch: 18/20... Step: 6010... Loss: 1.2437... Val Loss: 1.4291\n",
      "Epoch: 18/20... Step: 6020... Loss: 1.2799... Val Loss: 1.4354\n",
      "Epoch: 18/20... Step: 6030... Loss: 1.2865... Val Loss: 1.4351\n",
      "Epoch: 18/20... Step: 6040... Loss: 1.3134... Val Loss: 1.4322\n",
      "Epoch: 18/20... Step: 6050... Loss: 1.2967... Val Loss: 1.4293\n",
      "Epoch: 18/20... Step: 6060... Loss: 1.2861... Val Loss: 1.4330\n",
      "Epoch: 18/20... Step: 6070... Loss: 1.2601... Val Loss: 1.4281\n",
      "Epoch: 18/20... Step: 6080... Loss: 1.2702... Val Loss: 1.4292\n",
      "Epoch: 18/20... Step: 6090... Loss: 1.2686... Val Loss: 1.4257\n",
      "Epoch: 18/20... Step: 6100... Loss: 1.2665... Val Loss: 1.4250\n",
      "Epoch: 19/20... Step: 6110... Loss: 1.2502... Val Loss: 1.4404\n",
      "Epoch: 19/20... Step: 6120... Loss: 1.2989... Val Loss: 1.4272\n",
      "Epoch: 19/20... Step: 6130... Loss: 1.2541... Val Loss: 1.4282\n",
      "Epoch: 19/20... Step: 6140... Loss: 1.2332... Val Loss: 1.4309\n",
      "Epoch: 19/20... Step: 6150... Loss: 1.2537... Val Loss: 1.4313\n",
      "Epoch: 19/20... Step: 6160... Loss: 1.2753... Val Loss: 1.4290\n",
      "Epoch: 19/20... Step: 6170... Loss: 1.2797... Val Loss: 1.4288\n",
      "Epoch: 19/20... Step: 6180... Loss: 1.2589... Val Loss: 1.4356\n",
      "Epoch: 19/20... Step: 6190... Loss: 1.2333... Val Loss: 1.4348\n",
      "Epoch: 19/20... Step: 6200... Loss: 1.2467... Val Loss: 1.4388\n",
      "Epoch: 19/20... Step: 6210... Loss: 1.2578... Val Loss: 1.4324\n",
      "Epoch: 19/20... Step: 6220... Loss: 1.2343... Val Loss: 1.4296\n",
      "Epoch: 19/20... Step: 6230... Loss: 1.2505... Val Loss: 1.4310\n",
      "Epoch: 19/20... Step: 6240... Loss: 1.2699... Val Loss: 1.4291\n",
      "Epoch: 19/20... Step: 6250... Loss: 1.2418... Val Loss: 1.4267\n",
      "Epoch: 19/20... Step: 6260... Loss: 1.2607... Val Loss: 1.4317\n",
      "Epoch: 19/20... Step: 6270... Loss: 1.2538... Val Loss: 1.4386\n",
      "Epoch: 19/20... Step: 6280... Loss: 1.2559... Val Loss: 1.4351\n",
      "Epoch: 19/20... Step: 6290... Loss: 1.2701... Val Loss: 1.4234\n",
      "Epoch: 19/20... Step: 6300... Loss: 1.2861... Val Loss: 1.4307\n",
      "Epoch: 19/20... Step: 6310... Loss: 1.2576... Val Loss: 1.4287\n",
      "Epoch: 19/20... Step: 6320... Loss: 1.2649... Val Loss: 1.4289\n",
      "Epoch: 19/20... Step: 6330... Loss: 1.2570... Val Loss: 1.4260\n",
      "Epoch: 19/20... Step: 6340... Loss: 1.2511... Val Loss: 1.4335\n",
      "Epoch: 19/20... Step: 6350... Loss: 1.2659... Val Loss: 1.4279\n",
      "Epoch: 19/20... Step: 6360... Loss: 1.2467... Val Loss: 1.4304\n",
      "Epoch: 19/20... Step: 6370... Loss: 1.2768... Val Loss: 1.4319\n",
      "Epoch: 19/20... Step: 6380... Loss: 1.2947... Val Loss: 1.4324\n",
      "Epoch: 19/20... Step: 6390... Loss: 1.2959... Val Loss: 1.4348\n",
      "Epoch: 19/20... Step: 6400... Loss: 1.2726... Val Loss: 1.4283\n",
      "Epoch: 19/20... Step: 6410... Loss: 1.2375... Val Loss: 1.4293\n",
      "Epoch: 19/20... Step: 6420... Loss: 1.2562... Val Loss: 1.4236\n",
      "Epoch: 19/20... Step: 6430... Loss: 1.2643... Val Loss: 1.4267\n",
      "Epoch: 19/20... Step: 6440... Loss: 1.2659... Val Loss: 1.4232\n",
      "Epoch: 20/20... Step: 6450... Loss: 1.2568... Val Loss: 1.4352\n",
      "Epoch: 20/20... Step: 6460... Loss: 1.2982... Val Loss: 1.4334\n",
      "Epoch: 20/20... Step: 6470... Loss: 1.2676... Val Loss: 1.4268\n",
      "Epoch: 20/20... Step: 6480... Loss: 1.2142... Val Loss: 1.4301\n",
      "Epoch: 20/20... Step: 6490... Loss: 1.2685... Val Loss: 1.4247\n",
      "Epoch: 20/20... Step: 6500... Loss: 1.2423... Val Loss: 1.4296\n",
      "Epoch: 20/20... Step: 6510... Loss: 1.2570... Val Loss: 1.4281\n",
      "Epoch: 20/20... Step: 6520... Loss: 1.2716... Val Loss: 1.4283\n",
      "Epoch: 20/20... Step: 6530... Loss: 1.2432... Val Loss: 1.4315\n",
      "Epoch: 20/20... Step: 6540... Loss: 1.2168... Val Loss: 1.4274\n",
      "Epoch: 20/20... Step: 6550... Loss: 1.2412... Val Loss: 1.4268\n",
      "Epoch: 20/20... Step: 6560... Loss: 1.2581... Val Loss: 1.4248\n",
      "Epoch: 20/20... Step: 6570... Loss: 1.2597... Val Loss: 1.4266\n",
      "Epoch: 20/20... Step: 6580... Loss: 1.2499... Val Loss: 1.4220\n",
      "Epoch: 20/20... Step: 6590... Loss: 1.2744... Val Loss: 1.4302\n",
      "Epoch: 20/20... Step: 6600... Loss: 1.2915... Val Loss: 1.4257\n",
      "Epoch: 20/20... Step: 6610... Loss: 1.2632... Val Loss: 1.4289\n",
      "Epoch: 20/20... Step: 6620... Loss: 1.2559... Val Loss: 1.4289\n",
      "Epoch: 20/20... Step: 6630... Loss: 1.2598... Val Loss: 1.4252\n",
      "Epoch: 20/20... Step: 6640... Loss: 1.2718... Val Loss: 1.4236\n",
      "Epoch: 20/20... Step: 6650... Loss: 1.2874... Val Loss: 1.4257\n",
      "Epoch: 20/20... Step: 6660... Loss: 1.2829... Val Loss: 1.4238\n",
      "Epoch: 20/20... Step: 6670... Loss: 1.2562... Val Loss: 1.4249\n",
      "Epoch: 20/20... Step: 6680... Loss: 1.2700... Val Loss: 1.4331\n",
      "Epoch: 20/20... Step: 6690... Loss: 1.2490... Val Loss: 1.4323\n",
      "Epoch: 20/20... Step: 6700... Loss: 1.2643... Val Loss: 1.4332\n",
      "Epoch: 20/20... Step: 6710... Loss: 1.2457... Val Loss: 1.4318\n",
      "Epoch: 20/20... Step: 6720... Loss: 1.2683... Val Loss: 1.4299\n",
      "Epoch: 20/20... Step: 6730... Loss: 1.2911... Val Loss: 1.4295\n",
      "Epoch: 20/20... Step: 6740... Loss: 1.2702... Val Loss: 1.4280\n",
      "Epoch: 20/20... Step: 6750... Loss: 1.2313... Val Loss: 1.4184\n",
      "Epoch: 20/20... Step: 6760... Loss: 1.2627... Val Loss: 1.4245\n",
      "Epoch: 20/20... Step: 6770... Loss: 1.2481... Val Loss: 1.4254\n",
      "Epoch: 20/20... Step: 6780... Loss: 1.3348... Val Loss: 1.4262\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "\n",
    "train(net, encoded, epochs=20, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "piHStdJzmSfP"
   },
   "outputs": [],
   "source": [
    "model_name = 'rnn.net'\n",
    "\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7KBtcLSRpS9H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EN3NhGDZpcNo",
    "outputId": "fad42803-edf3-4eee-af14-9a0157d46385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8v-D1OSpcKG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvYsq-_gpcGc"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        \n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "SV897Tzgpfae",
    "outputId": "06f57e40-44a2-43c1-bc83-612491653845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Bruno Lima said,\n",
      "The son of Sheala and Ashan, all Abrahashan, the stild which will\n",
      "die an angel that said, Behold, thou shalt not.\n",
      "\n",
      "2:11 And he said, I shell be after this disinasion of thy father, in\n",
      "a man of thee, and shall be thee. And we have done with them.\n",
      "\n",
      "1:14 Afsering they had saw also, whose way they be and the see with\n",
      "them.\n",
      "\n",
      "429:13 And he had said, I will say unto you, I will come, and hath thine\n",
      "are with their hand and forgiven your shears, and that they was a\n",
      "good, and to the people shall break the companicions.\n",
      "\n",
      "23:20 Therefore, and he thou senteds all hand the children of Israel,\n",
      "and therefore will shek against the LORD, and all the serils of\n",
      "the whole, and will stand in the sons of Josah, saying, When thou shalt\n",
      "treaken to his face of the sacrifice of them: for they will saith\n",
      "us all that shall not, before thee, and walk in her son was brought\n",
      "to Jerusalem.\n",
      "\n",
      "1:24 And the presencs of the poreir of the callow also in the trouble thereof\n",
      "the sabettare, and were their fathers: and they shall be weal, and\n",
      "which thou hast seen the fathers.\n",
      "\n",
      "21:11 And it went offering to the word of the LORD.\n",
      "\n",
      "42:1 And all the priests and his son are as would serve him.\n",
      "\n",
      "2:5 And he had sand to Joram and the tentled things where it was\n",
      "the sons of the wife of the people in the corner of the servants, and with\n",
      "them were answered, and bring the proving as they would, and they\n",
      "will be not a prepering of Jerusalem, but he had brought\n",
      "him. That the camb of this discople was chorging into them the\n",
      "people.\n",
      "\n",
      "12:10 Therefore holder in trouth, and the LORD is compelled against\n",
      "him, and that thou have be to be as the children of Israel.\n",
      "\n",
      "14:3 And they sealled the whole that toward those thousand and\n",
      "seath, and said unto the Lirntel thou shalt they die that they shall\n",
      "set as the women also that hath be set upon the way of the son of Jehushabit.\n",
      "And hhe were so thise thousand wind.\n",
      "\n",
      "32:5 And they came a stood, and seally abonined the stares of the\n",
      "sons, that were; who was taken as them saying, and they\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, prime='And Bruno Lima said', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tA3300t-p8Sd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "PyTorch - CharRNN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
